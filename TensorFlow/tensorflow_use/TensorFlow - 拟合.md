## 过拟合与欠拟合

过拟合，就是数据过度匹配训练数据而无法很好的泛化到测试数据或新数据。出现过拟合的情况，通常就是训练周期过多。

欠拟合，就是数据还存在提高空间。欠拟合的出现原因很多，如模型不足，过于正则化，或没有训练足够长的时间。

过拟合是我们需要极力去避免的，也要了解如何去处理。通常，最好的方案就是使用更多的训练数据，但此方案在实际中不是经常能用的，那么就可以采用正则化技术，该类技术会限制模型存储的信息数量和类型，即主动的收敛网络能记住的模型，重点专注于最突出的，使这些模式能更好的泛化。


### 过拟合

要防止过拟合，最简单的方式就是缩小模型，即减少模型中可学习参数的数量（层数和每层单元数决定），该数量称为模型的容量，其容量越大，就越能完美映射训练样本与目标之间的关系，但这对泛化预测毫无意义。

另一方面，如果模型太小，将很难与训练数据拟合，为了最小化损失，就必须学习具有更强预测能力的压缩表示法。所以需要在容量太大与容量不足之间平衡。

然而，我们没有办法确定合适的模型大小或架构，这需要我们从相对较少的层和参数开始，然后逐步增加层的大小或添加新的层，直到看到返回的验证损失不断减小。

### 策略·权重正则化

权重正则化是基于限制网络复杂性产生，强制要求其权重采用较小的值，使权重值分布更“规则”，通过向网络的损失函数添加与权重较大相关的代价来实现。

- L1正则化，所添加的代价与权重系数的绝对值（L1范数）成正比。
- L2正则化，所添加的代价与权重系数的平方（L2范数）成正比。L2正则化在神经网络领域称为权重衰减。

权重正则化的添加方法是在层实例中通过参数kernel_regularizer传递。 

### 策略·丢弃层
丢弃是最有效且最常用的神经网络正则化技术之一。指在训练期间随机“丢弃”（设置为0）该层的多个输出特征。

丢弃率指变为0的特征所占的比例，通常在0.2-0.5。测试时，网络不会丢弃单元，而是将层的输出值按等同于丢弃率的比例缩减，用于平衡测试时的活跃单元数大于训练时的活跃单元数这一事实。


###总结
- 常见的过拟合避免侧率：
- 获取更多训练数据
- 降低网络容量
- 添加权重正则化
- 添加丢弃层
- **数据增强**，本节不提及
- **批次归一化**，本节不提及