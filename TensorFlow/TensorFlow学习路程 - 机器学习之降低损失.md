## TensorFlow学习路程 - 机器学习之降低损失 [【上页】](https://tinyworker.github.io/TensorFlow/index) ##
### 迭代方法(Recursive) ###
迭代方式是一种很基础的方式，即给予一个初始值，根据系统返回的损失，再不断尝试的过程，直到接近目标。而这个方式的难点，则在于**如何快速高效**的接近目标。

在经过多次迭代后，若总体损失变化极为缓慢或者不再变化时，可以说模型已**收敛**（即找到了最小损失）。

> 小记：由于迭代过程是固定的，即每次返回损失后下一次的初始值是恒定变化的，如果变化区间取值太大，可能会错过目标，太小，可能会导致多次迭代后依然没找到目标。所以，ML的难点大部分都集中在高效收敛这里。

### 梯度下降（Gradient Descent） ###
梯度下降法是ML中一个非常热门的机制，其基本思路是选择w1作为起始点，这个点是随机的；随后会计算起点的梯度（偏导数的矢量），而起点的损失对于梯度则相当于导数。

其中，梯度是矢量，所以具有方向和大小。梯度是始终指向损失函数中增长最为迅猛的方向的，所以梯度下降法总是沿着负梯度的方向走，以便尽快降低损失。

关于梯度移动，是依据当前点的梯度一部分与当前点相加，得到下一个点的位置，这个相加的一部分梯度就是步长，算法据此会不断重复直到收敛。

### 学习速率（Learning Ratio）###
学习速率在梯度下降法中，作为梯度乘积的常量使用，用于确定下个点的位置，前面提到了训练过程的难点在于高效，对于梯度下降法来说，选择合适的学习速率尤为重要，若速率过小，那可能很长时间都无法收敛，若速率过大，则可能越过最低点，在左右反复跳动。

针对此问题，通常在训练中有超参数一说，用于开发者随时调整速率，此外，在遇上样本量极大的数据集时，出现冗余的可能性就越高，因此有随机梯度下降法（SGD），通过单次迭代选择随机数量的随机样本进行训练。

> 小记，关于学习速率的选择，按照练习的情况来看是在一个常量范围中，由于该速率是参与到起点的梯度计算中的，因此数值范围可能会因为样本曲线不同而不同。




